{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Assignment 8: Document Retrieval"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this assignment, you're going to explore various similarity metrics we have learned so far and predict the label of a text document by assigning the label of its nearest neighbor to it. Due to the sparseness of document vectors (lots of zero entries), which results in a significantly large computational runtime, we will apply a method called Random Projection to reduce the dimensions of each document vector."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Fill in the cells provided marked `TODO` with code to answer the questions.\n",
                "\n",
                "\u003e Copyright ¬©2022 Pemi Nguyen.  All rights reserved.  Permission is hereby granted to students registered for University of Washington CSE/STAT 416 for use solely during Spring Quarter 2022 for purposes of the course.  No other use, copying, distribution, or modification is permitted without prior written consent. Copyrights for third-party components of this work must be honored.  Instructors interested in reusing these course materials should contact the author."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 1: Similarity Metrics\n",
                "\n",
                "In the first part, you will look at various metrics to measure similarity between text documents and think about their relative merits.\n",
                "\n",
                "The file `data.csv` contains information about 1000 articles. Each line in the file contains three fields: `(articleID, wordID, Count)` representing the number of times that word `wordID` occurs in article `articleID`. We don't store words that have zero frequencies in each article. No specific word names or article names are provided.\n",
                "\n",
                "The file `labels.csv` contains the `groupID`, the newsgroup that a given `articleID` belongs to (for a total of 1000 articles). The file `groups.csv` contains the specific name of each `groupID` (for a total of 20 newsgroups)."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load the datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "data = pd.read_csv('data.csv')\n",
                "groups = pd.read_csv('groups.csv')\n",
                "labels = pd.read_csv('labels.csv')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`data.csv` stores the non-zero frequencies of all words in each article."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "data"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`groups.csv` stores the names of 20 newsgroups."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "groups"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "`labels.csv` stores the newsgroup that each article belongs to."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "labels"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocessing the data"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Create an original matrix using the bag-of-words approach (since we stores the frequencies of words in each article). `scipy` provides tools for creating sparse matrices using multiple data structures, as well as tools for converting a dense matrix to a sparse matrix."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "import scipy.sparse as sparse\n",
                "original_matrix = sparse.csr_matrix((data.Count, (data.articleID, data.wordID))).toarray()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There are 1,000 articles and 61,067 words in the bag of words."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": [
                "original_matrix.shape"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We call this matrix sparse because it has a lot of zeroes in it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "original_matrix[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "# Set seed for the code\n",
                "np.random.seed(416)\n",
                "\n",
                "num_groups = len(groups)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "After , we're ready to use the new matrix where each document is represented by a 1000-dimensional vector."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Calcutating various similarity metrics"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We have learned four similarity metrics so far: Jaccard, Euclidean, Manhattan and Cosine. In Machine Learning, it's important to decide which metric you should use for your pipeline, but it's difficult to understand what these metrics do without visualizing its effects on our data.\n",
                "\n",
                "Note that Jaccard and Cosine similarity are numbers between $0$ and $1$, where as Manhattan and Euclidean between $-\\infty$ and $0$ (with higher numbers indicating more similarity).\n",
                "\n",
                "For questions 1-4, on EdStem, we will test your calculations on the following two vectors:\n",
                "\n",
                "```\n",
                "x = [1.4, 4.3, 2.5, 8.39]\n",
                "y = [17.3, 0.32, 9.21, 1.12]\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç **Question 1**: Calculate Jaccard similarity:\n",
                "\n",
                "The Jaccard similarity between two vector $x$ and $y$ is defined as:\n",
                "$$J(x, y) = \\frac{\\sum_i \\min(x_i, y_i)}{\\sum_i \\max(x_i, y_i)}$$\n",
                "\n",
                "Hint: To use `numpy` for this, you can concatenate these two arrays using np.concatenate  `np.vstack((x, y))`, then use `np.min(axis=0)`, `np.max(axis=0)` and `np.sum`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q1_jaccard) ###\n",
                "\n",
                "def jaccard_similarity(x, y):\n",
                "    \"\"\"\n",
                "    Calculate the Jaccard similarity between two vectors x and y\n",
                "\n",
                "    Args:\n",
                "    - x (np.array): The first vector\n",
                "    - y (np.array): The second vector\n",
                "\n",
                "    Returns:\n",
                "    - the Jaccard similarity between two vectors x and y\n",
                "    \"\"\"\n",
                "    # TODO\n",
                "    inter = np.vstack((x,y))\n",
                "    j = (np.sum(np.min(inter, axis=0)))/(np.sum(np.max(inter,axis=0)))\n",
                "    return j"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç **Question 2**: Calculate Euclidean similarity:\n",
                "\n",
                "The Euclidean similarity between two vector $x$ and $y$ is defined as:\n",
                "$$L_2(x, y) = - \\left\\| x- y\\right\\|_2 = - \\sqrt{\\sum_i (x_i - y_i)^2}$$\n",
                "\n",
                "Note: Euclidean similarity has a $-$ sign to indicate similarity (which is the opposite of Euclidean distance).\n",
                "\n",
                "Hint: Look into `np.linalg.norm`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q2_euclidean) ###\n",
                "\n",
                "def euclidean_similarity(x, y):\n",
                "    \"\"\"\n",
                "    Calculate the Euclidean similarity between two vectors x and y\n",
                "\n",
                "    Args:\n",
                "    - x (np.array): The first vector\n",
                "    - y (np.array): The second vector\n",
                "\n",
                "    Returns:\n",
                "    - the Euclidean similarity between two vectors x and y\n",
                "    \"\"\"\n",
                "    # TODO\n",
                "    e_similarity = -np.linalg.norm(x-y)\n",
                "    return e_similarity"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç **Question 3**: Calculate Manhattan similarity:\n",
                "\n",
                "The Manhattan similarity between two vector $x$ and $y$ is defined as:\n",
                "$$L_1(x, y) = - \\left\\| x- y\\right\\|_1 = - \\sum_i |x_i - y_i|$$\n",
                "\n",
                "Note: Same thing as Euclidean similarity, Manhattan has a $-$ sign to indicate similarity (which is the opposite of Manhattan distance).\n",
                "\n",
                "Hint: Look into `np.linalg.norm`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q3_manhattan) ###\n",
                "\n",
                "def manhattan_similarity(x, y):\n",
                "    \"\"\"\n",
                "    Calculate the Manhattan similarity between two vectors x and y\n",
                "\n",
                "    Args:\n",
                "    - x (np.array): The first vector\n",
                "    - y (np.array): The second vector\n",
                "\n",
                "    Returns:\n",
                "    - the Manhattan similarity between two vectors x and y\n",
                "    \"\"\"\n",
                "    # TODO\n",
                "    man = sum(-abs(val1-val2) for val1, val2 in zip(x,y))\n",
                "    return man"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç **Question 4**: Calculate Cosine similarity:\n",
                "\n",
                "The Cosine similarity between two vector $x$ and $y$ is defined as:\n",
                "$$S_c(x, y) = \\frac{\\sum_i x_i \\cdot y_i}{\\left\\| x\\right\\|_2 \\left\\| y\\right\\|_2}$$\n",
                "\n",
                "Hint: Look into `np.dot` (dot product between two vectors) and `np.linalg.norm`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q4_cosine) ###\n",
                "\n",
                "def cosine_similarity(x, y):\n",
                "    \"\"\"\n",
                "    Calculates the Cosine similarity between two vectors x and y\n",
                "    \n",
                "    Args:\n",
                "    - x (np.array): The first vector\n",
                "    - y (np.array): The second vector\n",
                "\n",
                "    Returns:\n",
                "    - the Cosine similarity between two vectors x and y\n",
                "    \"\"\"\n",
                "    # TODO\n",
                "    from numpy.linalg import norm\n",
                "    cos = np.dot(x,y)/(norm(x)*norm(y))\n",
                "    return(cos)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualizing group similarities and neartest neighbor document classification accuracies using a heatmap."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For each metric, we will visualize group similarities and neartest neighbor document classification accuracies with a heatmap. The plot will look like a $20 \\cdot 20$ matrix. Rows and columns are index by newsgroups\n",
                "(in the same order). A darker color indicates more similarity, and vice-versa."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import warnings\n",
                "\n",
                "# Heatmap\n",
                "def makeHeatMap(data, names, color, similarity_metric):\n",
                "#to catch \"falling back to Agg\" warning\n",
                "    with warnings.catch_warnings():\n",
                "        warnings.simplefilter(\"ignore\")\n",
                "        #code source: http://stackoverflow.com/questions/14391959/heatmap-in-matplotlib-with-pcolor\n",
                "        fig, ax = plt.subplots()\n",
                "        #create the map w/ color bar legend\n",
                "        heatmap = ax.pcolor(data, cmap=color)\n",
                "        cbar = plt.colorbar(heatmap)\n",
                "\n",
                "        # Set the size of the plots\n",
                "        fig = plt.gcf()\n",
                "        fig.set_size_inches(14, 11)\n",
                "\n",
                "        # put the major ticks at the middle of each cell\n",
                "        ax.set_xticks(np.arange(data.shape[0])+0.5, minor=False)\n",
                "        ax.set_yticks(np.arange(data.shape[1])+0.5, minor=False)\n",
                "\n",
                "        # want a more natural, table-like display\n",
                "        ax.invert_yaxis()\n",
                "        ax.xaxis.tick_top()\n",
                "        plt.xticks(rotation=90)\n",
                "\n",
                "        ax.set_xticklabels(names)\n",
                "        ax.set_yticklabels(names)\n",
                "        ax.set_title(similarity_metric)\n",
                "\n",
                "        plt.tight_layout()\n",
                "\n",
                "        # plt.savefig(outputFileName, format = 'png')\n",
                "        plt.show()\n",
                "        plt.close()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Calculating similaritities between every two newsgroups\n",
                "\n",
                "*Optional: This part takes 15 minutes to run, so you are not required to run it. Due to EdStem's limited computational limits, you cannot run the following cell. You can try running this notebook locally, or upload it on Google Colab for more computational usage.* \n",
                "\n",
                "We calculate the pairwise similarities between every two articles based on these 4 above metrics. After that, we then calculate pairwise similarities between every two newsgroup A and newsgroup B by averaging the similarity results between each article in A with each article in B.\n",
                "\n",
                "If you implement the similarity metric calculations correctly, here are the results:\n",
                "\n",
                "![alt text](https://drive.google.com/uc?export=view\u0026id=1QAUmPFLnCtzWNFRt7piq2Wy-E5cIC-lA)\n",
                "\n",
                "![alt text](https://drive.google.com/uc?export=view\u0026id=16eBriprMF2-4naTc8PIbwJbchpv6L3Uh)\n",
                "\n",
                "![alt text](https://drive.google.com/uc?export=view\u0026id=1cI5uaWTbjogiT9rGeVP6pjNFQJBzNhYJ)\n",
                "\n",
                "![alt text](https://drive.google.com/uc?export=view\u0026id=1Ukag4wE8oHE0oKyCByX0gnua7dDpuNK9)\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Uncomment the code (using `Cmd + /` if you're on a MacBook) to try running it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import itertools\n",
                "\n",
                "# similarity_matrix = np.zeros((4, num_groups, num_groups))\n",
                "# for i,j in itertools.combinations_with_replacement(groups.index, 2):\n",
                "#     articles_i = labels[labels.label == i].index\n",
                "#     articles_j = labels[labels.label == j].index\n",
                "#     pairwise_results = np.zeros((4, len(articles_i)*len(articles_j)))\n",
                "#     k = 0\n",
                "#     for a,b in itertools.product(articles_i, articles_j):\n",
                "#         pairwise_results[0, k] = jaccard_similarity(original_matrix[a], original_matrix[b])\n",
                "#         pairwise_results[1, k] = euclidean_similarity(original_matrix[a], original_matrix[b])\n",
                "#         pairwise_results[2, k] = manhattan_similarity(original_matrix[a], original_matrix[b])\n",
                "#         pairwise_results[3, k] = cosine_similarity(original_matrix[a], original_matrix[b])\n",
                "#         k += 1\n",
                "#     similarity_matrix[:,i,j] = similarity_matrix[:,j,i] = pairwise_results.mean(axis=1)\n",
                "\n",
                "# makeHeatMap(similarity_matrix[0], groups.group_name, 'Blues', 'Jaccard Similarity between 20 newsgroups')\n",
                "# makeHeatMap(similarity_matrix[1], groups.group_name, 'Blues', 'Euclidean Similarity between 20 newsgroups')\n",
                "# makeHeatMap(similarity_matrix[2], groups.group_name, 'Blues', 'Manhattan Similarity between 20 newsgroups')\n",
                "# makeHeatMap(similarity_matrix[3], groups.group_name, 'Blues', 'Cosine Similarity between 20 newsgroups')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 2: Nearest neighbor classification"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A nearest-neighbor classification system is conceptually extremely simple, and often is very effective. Given a large dataset of labeled examples, a nearest-neighbor classification system will predict a label for a new\n",
                "example, $x$, as follows: It will find the element of the labeled dataset that is closest to $x$ ‚Äî closest in whatever metric makes the most sense for that dataset ‚Äî and then output the label of this closest point."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparing between above 4 metrics\n",
                "\n",
                "Of the 4 metrics above, it seems like Manhattan and Euclidean metrics are pretty off, since the heatmap is dark at the majority of cells, which suggests a lot of these 20 different newsgroups are pretty similar to each other (which doesn't make sense). Besides, the fact that the range of these two metrics are unbounded ($-\\infty$ to $0$) is also a disadvantage.\n",
                "\n",
                "Both Jaccard and Cosine heatmaps are more nuanced, since their range is bounded (between 0 and 1). Looking at their heatmaps, we can see there is a wide distribution of color shades from white to dark blue across different cells. Between these two, Cosine seems better because the middle range of values on the legends for Jaccard is quite limited (0.05 to 0.13), but that for Cosine is wider (0.20 to 0.50).\n",
                "\n",
                "As a result, for this problem, we will choose Cosine Similarity metric for our nearest neighbor classification task. In general, in NLP, to measure similarity between document vectors, Cosine similarity is more preferred because of another nice interpretation: It represents the angle between two vectors."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Nearest neighbor search"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's implement our nearest neighbor search algorithm. From a vector that represents an article, we will find another article that has the highest similarity to it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "def nearest_neighbor_search(article_vector, data_matrix, articleID, metric=cosine_similarity):\n",
                "    \"\"\"\n",
                "    For each article, return the articleID of the article most similar to it (excluding itself)\n",
                "    according to a similarity metric\n",
                "    \n",
                "    Args:\n",
                "    - article_vector (np.array): A numpy array that stores the vector for an article\n",
                "    - data_matrix (np.array): A numpy array which is the matrix representation for all articles\n",
                "    - articleID (int): The articleID of an article that we are querying for its nearest neighbor.\n",
                "    - metric (func): The function of a similarity metric (from 4 choices above). \n",
                "    By default, we're choosing cosine_similarity. You can also choose manhattan_similarity, euclidean_similarity\n",
                "    and jaccard_similarity.\n",
                "\n",
                "    Returns:\n",
                "    - The articleID of the nearest neighbor article.\n",
                "\n",
                "    \"\"\"\n",
                "    # Calculating the similarity score between a specific article and all the articles\n",
                "    similarity_vec = np.apply_along_axis(lambda row: metric(row, article_vector), 1, data_matrix)\n",
                "\n",
                "    # Set similarity result at the index of the current article to be -inf to avoid returning it\n",
                "    similarity_vec[articleID] = np.NINF\n",
                "\n",
                "    return np.nanargmax(similarity_vec)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Classification Matrix"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Build a `classification_matrix` in which the columns represent the true labels and the columns represent predictions. A cell value at index `(i, j)`, particularly `classification_matrix[i, j]`, indicates the number of articles which have true labels with group_ID `i` and predictions with group_ID `j`. \n",
                "\n",
                "For example, for a matrix:\n",
                "\n",
                "```\n",
                "[[3, 2, 0],\n",
                "[1, 0, 0], \n",
                "[4, 0, 0]]\n",
                "```\n",
                "\n",
                "At column 0 and row 2, there are 4 articles whose true labels are `groupID = 2` and predictions whose predicted labels are `groupID = 0`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_classification_matrix(data_matrix, groups=groups, labels=labels, metric=cosine_similarity):\n",
                "    \"\"\"\n",
                "    Build a classification matrix in which the columns represent the true labels \n",
                "    and the columns represent predictions. \n",
                "    A cell value at index `(i, j)`, particularly `classification_matrix[i, j]`, \n",
                "    indicates the number of articles which have true labels with group_ID `i` \n",
                "    and predictions with group_ID `j`. \n",
                "    \n",
                "    Args:\n",
                "    - data_matrix (np.array): A numpy array which is the matrix representation for all articles\n",
                "    - groups (pd.DataFrame): A DataFrame storing information about different groups of a dataset\n",
                "    - labels (pd.DataFrame): A DataFrame storing information about the labels for all articles\n",
                "    - metric (func): The function of a similarity metric (from 4 choices above). \n",
                "    By default, we're choosing cosine_similarity. You can also choose manhattan_similarity, euclidean_similarity\n",
                "    and jaccard_similarity.\n",
                "\n",
                "    Returns:\n",
                "    - A classification matrix\n",
                "\n",
                "    \"\"\"\n",
                "    classification_matrix = np.zeros((len(groups),len(groups)))\n",
                "\n",
                "    for i in range(len(labels)):\n",
                "        true_label = labels.loc[i,'label']\n",
                "        doc = data_matrix[i]\n",
                "        most_sim_doc = nearest_neighbor_search(doc, data_matrix, i, metric)\n",
                "        most_sim_label = labels.loc[most_sim_doc,'label']\n",
                "        classification_matrix[true_label,most_sim_label] += 1\n",
                "    \n",
                "    return classification_matrix"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualizing classification matrix using original dimensions\n",
                "\n",
                "*Optional: This part takes 10 minutes to run, so you are not required to run it. Due to EdStem's limited computational limits, you cannot run the following cell. You can try running this notebook locally, or upload it on Google Colab for more computational usage.* \n",
                "\n",
                "Here's the heatmap for this matrix if you don't wish to run the code:\n",
                "\n",
                "![alt text](https://drive.google.com/uc?export=view\u0026id=1Ownsmdy_2QWBB1yG9FzSczlk8Sspgnx4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "# classification_matrix_original = build_classification_matrix(original_matrix)\n",
                "# makeHeatMap(classification_matrix_original, groups.group_name, 'Blues', 'Classification Matrix with original dimensions')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### üîç **Question 5**: Calculate total classfication accuracy"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Based on a classification matrix, let's calculate the total classification accuracy.\n",
                "\n",
                "For example, for this matrix:\n",
                "\n",
                "```\n",
                "[[3, 2, 0],\n",
                "[1, 0, 0],\n",
                "[4, 0, 0]]\n",
                "```\n",
                "\n",
                "the total accuracy is = Total correct predictions / Total number of examples\n",
                "\n",
                "$$= \\frac{3 + 0 + 0}{3 + 2 + 0 + 1 + 0 + 0 + 4 + 0+0} = 0.3$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_q5_classification_accuracy) ###\n",
                "\n",
                "def compute_classification_accuracy(classification_matrix):\n",
                "    \"\"\"\n",
                "    Computes classification accuracy from a classification matrix\n",
                "    \n",
                "    Args:\n",
                "    - classification matrix (np.array): A classification matrix\n",
                "\n",
                "    Returns:\n",
                "    - The classification accuracy for a classification matrix.\n",
                "    \"\"\"\n",
                "    \n",
                "    # TODO\n",
                "    correct = np.sum(np.diag(classification_matrix))\n",
                "    tot = np.sum(classification_matrix)\n",
                "    accuracy = correct/tot\n",
                "    return accuracy"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For the classsification matrix with the original dimensions, the total classification accuracy is $0.456$. If you uncomment the above code, uncomment this one below too to see how the code runs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "# compute_classification_accuracy(classification_matrix_original)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dimensionality Reduction using Random Projection"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It takes lots of computation resources for dealing with 61k dimensions. Aside from PCA, another technique that can help us reduce the dimensions of an original matrix is **Random Projection**.\n",
                "\n",
                "Random Projection: Given a set of $k$-dimensional vectors ${v_1, v_2, ...}$, define a $d \\times  k$ matrix $M$ by drawing each entry randomly (and independently) from a normal distribution of mean $0$ and variance\n",
                "$1$. The $d$-dimensional reduced vector corresponding to $v_i$\n",
                "is given by the matrix-vector product $Mv_i$\n",
                "\n",
                "We can think of the matrix $M$ as a set of $d$ random $k$-dimensional vectors ${w_1,...w_d}$ (the rows of $M$), and then the $j$-th coordinate of the reduced vector $Mv_i$ is the inner product between that $v_i$ and $w_j$."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Try various reduced dimensions $[10, 50, 100, 200, 500]$ to see how the classification accuracy improves and how the heatmap differs.\n",
                "\n",
                "Note: EdStem's kernel will stop if you choose `k \u003e 500` due to EdStem's computational limitations. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [],
            "source": [
                "ks = [10, 50, 100, 200, 500]\n",
                "\n",
                "classification_matrix_reduced = np.zeros((len(ks), num_groups, num_groups))\n",
                "classification_accuracies = []\n",
                "\n",
                "m = original_matrix.shape[1]\n",
                "for idx, k in enumerate(ks):\n",
                "    random_matrix = np.random.normal(size=(m,k))\n",
                "    projected_matrix = original_matrix @ random_matrix\n",
                "    classification_matrix_reduced[idx] = build_classification_matrix(projected_matrix)\n",
                "    current_accuracy = compute_classification_accuracy(classification_matrix_reduced[idx])\n",
                "    classification_accuracies.append(current_accuracy)\n",
                "    print(\"Nearest neighbor classification accuracy with k = \", k, \": \", current_accuracy)\n",
                "    makeHeatMap(classification_matrix_reduced[idx], groups.group_name, 'Blues', 'Dimensionality Reduction Classification (k={})'.format(k))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Visualizing the improvement of classification accuracy with increasing number of reduced dimensions"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can see from above that using $k = 500$, the accuracy can match closely to the accuracy when using the original number of dimensions $d = 61,067$. As we use more dimensions, the classification accuracy seems to improve. However, there is a clear tradeoff between the amount of improvement and computational efficiency."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(ks, classification_accuracies)\n",
                "plt.xlabel(\"Number of reduced dimensions\")\n",
                "plt.ylabel(\"Classification accuracy\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualizing classification matrices using other metrics"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see why Cosine similiarity is the best out of all metrics by comparing the classification accuracies with the same number of reduced dimensions."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Jaccard Similarity"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Clearly the accuracies are pretty low here. The heatmaps are also supposed to be dark along the diagonal to indicate high classification accuracies, which isn't the case for Jaccard."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "ks = [10, 50, 100]\n",
                "\n",
                "classification_matrix_reduced = np.zeros((len(ks), num_groups, num_groups))\n",
                "classification_accuracies = []\n",
                "\n",
                "m = original_matrix.shape[1]\n",
                "for idx, k in enumerate(ks):\n",
                "    random_matrix = np.random.normal(size=(m,k))\n",
                "    projected_matrix = original_matrix @ random_matrix\n",
                "    classification_matrix_reduced[idx] = build_classification_matrix(projected_matrix, metric=jaccard_similarity)\n",
                "    current_accuracy = compute_classification_accuracy(classification_matrix_reduced[idx])\n",
                "    classification_accuracies.append(current_accuracy)\n",
                "    print(\"Nearest neighbor classification accuracy with k = \", k, \": \", current_accuracy)\n",
                "    makeHeatMap(classification_matrix_reduced[idx], groups.group_name, 'Blues', 'Dimensionality Reduction Classification (k={})'.format(k))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Manhattan Similarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "ks = [10, 50, 100]\n",
                "\n",
                "classification_matrix_reduced = np.zeros((len(ks), num_groups, num_groups))\n",
                "classification_accuracies = []\n",
                "\n",
                "m = original_matrix.shape[1]\n",
                "for idx, k in enumerate(ks):\n",
                "    random_matrix = np.random.normal(size=(m,k))\n",
                "    projected_matrix = original_matrix @ random_matrix\n",
                "    classification_matrix_reduced[idx] = build_classification_matrix(projected_matrix, metric=manhattan_similarity)\n",
                "    current_accuracy = compute_classification_accuracy(classification_matrix_reduced[idx])\n",
                "    classification_accuracies.append(current_accuracy)\n",
                "    print(\"Nearest neighbor classification accuracy with k = \", k, \": \", current_accuracy)\n",
                "    makeHeatMap(classification_matrix_reduced[idx], groups.group_name, 'Blues', 'Dimensionality Reduction Classification (k={})'.format(k))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Euclidean Similarity"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Both Euclidean and Manhattan similarities have much better accuracies than Jaccard one, but they're not as good as Cosine one for the same number of reduced dimensions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "ks = [10, 50, 100]\n",
                "\n",
                "classification_matrix_reduced = np.zeros((len(ks), num_groups, num_groups))\n",
                "classification_accuracies = []\n",
                "\n",
                "m = original_matrix.shape[1]\n",
                "for idx, k in enumerate(ks):\n",
                "    random_matrix = np.random.normal(size=(m,k))\n",
                "    projected_matrix = original_matrix @ random_matrix\n",
                "    classification_matrix_reduced[idx] = build_classification_matrix(projected_matrix, metric=euclidean_similarity)\n",
                "    current_accuracy = compute_classification_accuracy(classification_matrix_reduced[idx])\n",
                "    classification_accuracies.append(current_accuracy)\n",
                "    print(\"Nearest neighbor classification accuracy with k = \", k, \": \", current_accuracy)\n",
                "    makeHeatMap(classification_matrix_reduced[idx], groups.group_name, 'Blues', 'Dimensionality Reduction Classification (k={})'.format(k))"
            ]
        }
    ]
}
